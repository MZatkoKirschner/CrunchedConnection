{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read merged flight+met data for ML and perform slight processing\n",
    "file2019 = '../data/processed/merged/2019_FlightMetMerged.csv'\n",
    "df2019 = pd.read_csv(file2019)\n",
    "\n",
    "file2018 = '../data/processed/merged/2018_FlightMetMerged.csv'\n",
    "df2018 = pd.read_csv(file2018)\n",
    "\n",
    "file2017 = '../data/processed/merged/2017_FlightMetMerged.csv'\n",
    "df2017 = pd.read_csv(file2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine 2018 and 2019\n",
    "dftmp = [df2017,df2018,df2019]\n",
    "df = pd.concat(dftmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read merged flight+met data for ML and perform slight processing\n",
    "#file2019 = '../data/processed/merged/2019_FlightMetMerged.csv'\n",
    "#df = pd.read_csv(file2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Read in airport region data (region by timezone)\n",
    "regionFile = '../data/raw/met/airport_regions.csv'\n",
    "regions = pd.read_csv(regionFile,skiprows=1)\n",
    "\n",
    "regions['airport'] = 'K' + regions['iata_code']\n",
    "regions['airport'].loc[regions['airport']=='KANC'] = 'PANC' #Anchorage\n",
    "regions['airport'].loc[regions['airport']=='KHNL'] = 'PHLN' #Honolulu\n",
    "\n",
    "regions.drop(columns=['iata_code','iana_tz','windows_tz'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where cancellations are not due to weather\n",
    "dfcc = df[ (df['CANCELLATION_CODE']!='A') & (df['CANCELLATION_CODE']!='C') & (df['CANCELLATION_CODE']!='D') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Prevent cancelled flights due to weather from being excluded\n",
    "dfcc.loc[(dfcc['CANCELLATION_CODE']=='B'),'ARR_DELAY_GROUP'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where there is a delay that isn't at least partially due to severe weather\n",
    "dfccOnlyWxDelay = dfcc[~(dfcc['WEATHER_DELAY']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure flights cancelled because of weather are still in here\n",
    "dfccOnlyWxDelay.loc[(dfccOnlyWxDelay['CANCELLATION_CODE']=='B'),'ARR_DELAY_GROUP'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "dfccOnlyWxDelay.drop(columns=['CANCELLED','DIVERTED','WEATHER_DELAY','NAS_DELAY','LATE_AIRCRAFT_DELAY','CANCELLATION_CODE'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For df used for ML modeling \n",
    "dfML = dfccOnlyWxDelay.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML[dfML['ARR_DELAY_GROUP']>0].to_csv('2017-2019_WxDelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-830a13bc9718>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']<=0)] = 0\n",
      "<ipython-input-13-830a13bc9718>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']>0)] = 1\n"
     ]
    }
   ],
   "source": [
    "#Merged arrival bins for simplicity for now\n",
    "\n",
    "#Less than 15 minutes late (which is considered on-time)\n",
    "dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']<=0)] = 0\n",
    "\n",
    "#More than 15 minutes late\n",
    "dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']>0)] = 1\n",
    "\n",
    "#dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']<=0)] = 0\n",
    "#dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']>0) & (dfML['ARR_DELAY_GROUP']<4)] = 1\n",
    "#dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']>=4) & (dfML['ARR_DELAY_GROUP']<8)] = 2\n",
    "#dfML['ARR_DELAY_GROUP'].loc[(dfML['ARR_DELAY_GROUP']>=8)] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge so airport region-by-timezone\n",
    "result = pd.merge(dfML,regions,left_on=['ORIGIN'],right_on=['airport'])\n",
    "result.drop('airport',axis=1,inplace=True)\n",
    "result.rename(columns={\"region\":\"ORIG_REGION\"},inplace=True)\n",
    "\n",
    "dfMLmerged = pd.merge(result,regions,left_on=['DEST'],right_on=['airport'])\n",
    "dfMLmerged.drop('airport',axis=1,inplace=True)\n",
    "dfMLmerged.rename(columns={\"region\":\"DEST_REGION\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save merged dataset to hdf5 file\n",
    "dfMLmerged.to_hdf('MergedFlightMet_2017-2019_withRegion_BinnedArrGroup.h5',key='dfMLmerged',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfMLmerged = dfML.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read hdf5 file\n",
    "file = 'MergedFlightMet_2017-2019_withRegion_BinnedArrGroup.h5'\n",
    "dfMLmerged = pd.read_hdf(file,key='dfMLmerged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert non-numeric to numeric\n",
    "dfMLmerged['OP_UNIQUE_CARRIER'].replace({'MQ':1,'AA':2,'B6':3,'OO':4,'OH':5,'UA':6,'AS':7,'NK':8,\n",
    "                                 'YX':9,'YV':10,'WN':11,'DL':12,'EV':13,'9E':14,'F9':15,'HA':16,'G4':17,'VX':18},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMLmerged['DEP_TIME'] = pd.to_datetime(dfMLmerged['DEP_TIME'])\n",
    "dfMLmerged['ARR_TIME'] = pd.to_datetime(dfMLmerged['ARR_TIME'])\n",
    "\n",
    "#Convert date infomation to feature info\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.month==12) | (dfMLmerged['DEP_TIME'].dt.month==1) | (dfMLmerged['DEP_TIME'].dt.month==2),'SEASON'] = 1\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.month==3) | (dfMLmerged['DEP_TIME'].dt.month==4) | (dfMLmerged['DEP_TIME'].dt.month==5),'SEASON'] = 2\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.month==6) | (dfMLmerged['DEP_TIME'].dt.month==7) | (dfMLmerged['DEP_TIME'].dt.month==8),'SEASON'] = 3\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.month==9) | (dfMLmerged['DEP_TIME'].dt.month==10) | (dfMLmerged['DEP_TIME'].dt.month==11),'SEASON'] = 4\n",
    "\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.hour>=1) | (dfMLmerged['DEP_TIME'].dt.hour<=8),'TOD'] = 1\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.hour>=9) | (dfMLmerged['DEP_TIME'].dt.hour<=14),'TOD'] = 2\n",
    "dfMLmerged.loc[(dfMLmerged['DEP_TIME'].dt.hour>=15) | (dfMLmerged['DEP_TIME'].dt.hour<=23),'TOD'] = 3\n",
    "\n",
    "dfMLmerged['month'] = dfMLmerged['DEP_TIME'].dt.month\n",
    "dfMLmerged['hour'] = dfMLmerged['DEP_TIME'].dt.hour\n",
    "dfMLmerged['DOW'] = dfMLmerged['DEP_TIME'].dt.dayofweek\n",
    "dfMLmerged['DOY'] = dfMLmerged['DEP_TIME'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder from sklearn \n",
    "#from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# generate binary values using get_dummies\n",
    "#dum_df = pd.get_dummies(dfMLmerged, columns=[\"OP_UNIQUE_CARRIER\"], prefix=[\"Carrier\"])\n",
    "\n",
    "#dum_df2 = pd.get_dummies(dum_df, columns=['ORIG_REGION'],prefix=['Orig_Region'])\n",
    "\n",
    "#dum_df3 = pd.get_dummies(dum_df2, columns=['DEST_REGION'],prefix=['Dest_Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save merged dataset to hdf5 file\n",
    "#dum_df3.to_hdf('MergedFlightMet_2017-2019_withRegion_BinnedArrGroup_OneHotEncoding.h5',key='data',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dfML[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'ceil_D','ceil_A','visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D']]\n",
    "\n",
    "#X = dfML[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'ceil_D','ceil_A','visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'OP_UNIQUE_CARRIER','DISTANCE_GROUP','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','SEASON']]\n",
    "\n",
    "#X = dfML[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'ceil_D','ceil_A','visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'OP_UNIQUE_CARRIER','DISTANCE_GROUP','SEASON','TOD']]\n",
    "\n",
    "#X = dfML[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'ceil_D','ceil_A','visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'OP_UNIQUE_CARRIER','DISTANCE_GROUP','month','hour','DOW','DOY','ORIGIN-DEST']]\n",
    "\n",
    "#X = dfML[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'tmpF_A', \n",
    "#          'visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'DISTANCE_GROUP','month','hour','DOY']]\n",
    "\n",
    "#X2019 = dfML2019[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'tmpF_A', \n",
    "#          'visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'DISTANCE_GROUP','month','hour','DOY']]\n",
    "\n",
    "#X = dfMLmerged[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'ceil_D','ceil_A','visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'OP_UNIQUE_CARRIER','DISTANCE_GROUP','month','hour','DOW','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID']]\n",
    "\n",
    "X = dfMLmerged[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "       '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', '6hrTsPrb_15mi_A',\n",
    "       '6hrSvrTsPrb_25mi_A', '6hPrecPrb_D', '6hPrecPrb_A', '12hPrecPrb_D',\n",
    "       '12hPrecPrb_A', 'snow_D', 'snow_A', 'spd_A', 'tmpF_D', 'dptF_D',\n",
    "       'tmpF_A', 'dptF_A', 'ceil_D', 'ceil_A', 'visib_D', 'visib_A',\n",
    "       '6hQntPrec_D', '6hQntPrec_A', '12hQntPrec_A', '12hQntPrec_D',\n",
    "       'OP_UNIQUE_CARRIER', 'DISTANCE_GROUP', 'month', 'hour', 'DOY']]\n",
    "\n",
    "#X = dfMLmerged[['spd_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','spd_A','tmpF_D', 'dptF_D','tmpF_A', 'dptF_A', \n",
    "#          'OP_UNIQUE_CARRIER','DISTANCE_GROUP','SEASON','TOD','DOW']]\n",
    "\n",
    "# X = dum_df3[['spd_D', 'fzRnPrb_D', 'snowPrb_D', '6hrTsPrb_15mi_D',\n",
    "#          '6hrSvrTsPrb_25mi_D', 'fzRnPrb_A', 'snowPrb_A', \n",
    "#          '6hrTsPrb_15mi_A','6hrSvrTsPrb_25mi_A','6hPrecPrb_D','6hPrecPrb_A','12hPrecPrb_D',\n",
    "#          '12hPrecPrb_A','snow_D','snow_A','spd_A','tmpF_D', 'tmpF_A', \n",
    "#          'visib_D','visib_A','6hQntPrec_D','6hQntPrec_A','12hQntPrec_A','12hQntPrec_D',\n",
    "#          'DISTANCE_GROUP','month','hour','DOY','Carrier_1','Carrier_2','Carrier_3','Carrier_4',\n",
    "#         'Carrier_5','Carrier_6','Carrier_7','Carrier_8','Carrier_9','Carrier_10','Carrier_11',\n",
    "#         'Carrier_12','Carrier_13','Carrier_14','Carrier_15','Carrier_17','Carrier_18',\n",
    "#            'Orig_Region_2','Orig_Region_5','Orig_Region_6','Orig_Region_7','Orig_Region_8',\n",
    "#            'Dest_Region_2','Dest_Region_5','Dest_Region_6','Dest_Region_7','Dest_Region_8']]\n",
    "\n",
    "# y = dum_df3['ARR_DELAY_GROUP']\n",
    "\n",
    "y = dfMLmerged['ARR_DELAY_GROUP']\n",
    "\n",
    "#y2019 = dfML2019['ARR_DELAY_GROUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in log of variables too\n",
    "# X['spd_D_log'] = np.log(X['spd_D']+1)\n",
    "# X['spd_A_log'] = np.log(X['spd_A']+1)\n",
    "\n",
    "# X['6hPrecPrb_D_log'] = np.log(X['6hPrecPrb_D']+1)\n",
    "# X['6hPrecPrb_A_log'] = np.log(X['6hPrecPrb_A']+1)\n",
    "\n",
    "# X['fzRnPrb_D_log'] = np.log(X['fzRnPrb_D']+1)\n",
    "# X['fzRnPrb_A_log'] = np.log(X['fzRnPrb_A']+1)\n",
    "\n",
    "# X['snowPrb_D_log'] = np.log(X['snowPrb_D']+1)\n",
    "# X['snowPrb_A_log'] = np.log(X['snowPrb_A']+1)\n",
    "\n",
    "# X['6hrTsPrb_15mi_D_log'] = np.log(X['6hrTsPrb_15mi_D']+1)\n",
    "# X['6hrTsPrb_15mi_A_log'] = np.log(X['6hrTsPrb_15mi_A']+1)\n",
    "\n",
    "# X['6hrSvrTsPrb_25mi_D_log'] = np.log(X['6hrSvrTsPrb_25mi_D']+1)\n",
    "# X['6hrSvrTsPrb_25mi_A_log'] = np.log(X['6hrSvrTsPrb_25mi_A']+1)\n",
    "\n",
    "# X['12hPrecPrb_D_log'] = np.log(X['12hPrecPrb_D']+1)\n",
    "# X['12hPrecPrb_A_log'] = np.log(X['12hPrecPrb_A']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split conversion dataset into train and test groups\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = [X_test,y_test]\n",
    "#test = pd.concat(tmp,axis=1)\n",
    "test1 = test[test['ARR_DELAY_GROUP']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.to_csv('ytest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2019_train, X2019_test, y2019_train, y2019_test = train_test_split(X2019, y2019,stratify=y2019,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing the data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy='minority',random_state=42)\n",
    "\n",
    "# fit and apply the transform\n",
    "X_train_over, y_train_over = over.fit_resample(X_train, y_train)\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy='majority',random_state=42)\n",
    "\n",
    "# fit and apply the transform\n",
    "X_train_under, y_train_under = under.fit_resample(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More balancing of data\n",
    "\n",
    "# define undersampling strategy\n",
    "#under = RandomUnderSampler(sampling_strategy='majority',random_state=42)\n",
    "\n",
    "# fit and apply the transform\n",
    "#X_train_under, y_train_under = under.fit_resample(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_train)\n",
    "#X_train_scale = scaler.transform(X_train)\n",
    "#X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train random forest model\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10,n_jobs=5,class_weight='balanced',random_state=42,max_features=10).fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"pickle_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-99-fd0e9efe391d>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-fd0e9efe391d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    pickle.dump(clf, file)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#Save model to pickle file\n",
    "#pkl_filename = 'pickle_model.pkl'\n",
    "#with open(pkl_filename, 'wb') as file\n",
    "\n",
    "#saved_model = pickle.dumps(clf)\n",
    "\n",
    "#saved_model = pickle.dumps(knn)\n",
    "\n",
    "#filename = 'finalized_model.sav'\n",
    "#pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "#    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/insight/lib/python3.8/site-packages/lightgbm/__init__.py:42: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  warnings.warn(\"Starting from version 2.2.1, the library file in distribution wheels for macOS \"\n"
     ]
    }
   ],
   "source": [
    "#Try out XGBoost\n",
    "#from xgboost import XGBClassifier\n",
    "#model_xgb = XGBClassifier(random_state=42,n_estimators=10).fit(X_train_under, y_train_under)\n",
    "\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#gb_clf = GradientBoostingClassifier(n_estimators=20,random_state=42).fit(X_train_under, y_train_under)\n",
    "\n",
    "#import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run trained model on test set\n",
    "forest_predicted = clf.predict(X_test)\n",
    "\n",
    "#forest_predicted = gb_clf.predict(X_test)\n",
    "\n",
    "#forest_predicted = model_xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99   3854090\n",
      "           1       0.73      0.43      0.54     94593\n",
      "\n",
      "    accuracy                           0.98   3948683\n",
      "   macro avg       0.86      0.71      0.77   3948683\n",
      "weighted avg       0.98      0.98      0.98   3948683\n",
      "\n",
      " \n",
      "Micro-averaged precision = 0.98 (treat instances equally)\n",
      "Macro-averaged precision = 0.86 (treat classes equally)\n",
      "Micro-averaged f1 = 0.98 (treat instances equally)\n",
      "Macro-averaged f1 = 0.77 (treat classes equally)\n",
      " \n",
      "Matthews correlation coefficient = 0.55\n",
      "Cohen kappa score = 0.54\n",
      " \n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3838637,   15453],\n",
       "       [  53541,   41052]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print('Classification Report')\n",
    "#print(classification_report(y_test, forest_predicted,target_names=['< 15 min', '15-1hr','1hr-2hr','>2 hr']))\n",
    "print(classification_report(y_test, forest_predicted,target_names=['0', '1']))\n",
    "\n",
    "print (' ')\n",
    "\n",
    "print('Micro-averaged precision = {:.2f} (treat instances equally)'\n",
    "       .format(precision_score(y_test, forest_predicted, average = 'micro')))\n",
    "print('Macro-averaged precision = {:.2f} (treat classes equally)'\n",
    "      .format(precision_score(y_test, forest_predicted, average = 'macro')))\n",
    "print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n",
    "      .format(f1_score(y_test, forest_predicted, average = 'micro')))\n",
    "print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n",
    "      .format(f1_score(y_test, forest_predicted, average = 'macro')))\n",
    "\n",
    "print (' ')\n",
    "\n",
    "print('Matthews correlation coefficient = {:.2f}'\n",
    "      .format(matthews_corrcoef(y_test, forest_predicted)))\n",
    "\n",
    "print('Cohen kappa score = {:.2f}'\n",
    "      .format(cohen_kappa_score(y_test, forest_predicted)))\n",
    "\n",
    "print (' ')\n",
    "print ('Confusion Matrix')\n",
    "confusion_matrix(y_test, forest_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: 6hrTsPrb_15mi_D Importance: 0.06\n",
      "Variable: 6hPrecPrb_D  Importance: 0.06\n",
      "Variable: 12hPrecPrb_D Importance: 0.06\n",
      "Variable: tmpF_D       Importance: 0.06\n",
      "Variable: dptF_D       Importance: 0.06\n",
      "Variable: DOY          Importance: 0.06\n",
      "Variable: 6hPrecPrb_A  Importance: 0.05\n",
      "Variable: spd_D        Importance: 0.04\n",
      "Variable: 6hrSvrTsPrb_25mi_D Importance: 0.04\n",
      "Variable: spd_A        Importance: 0.04\n",
      "Variable: tmpF_A       Importance: 0.04\n",
      "Variable: dptF_A       Importance: 0.04\n",
      "Variable: 6hQntPrec_D  Importance: 0.04\n",
      "Variable: OP_UNIQUE_CARRIER Importance: 0.04\n",
      "Variable: hour         Importance: 0.04\n",
      "Variable: 6hrTsPrb_15mi_A Importance: 0.03\n",
      "Variable: 6hrSvrTsPrb_25mi_A Importance: 0.03\n",
      "Variable: 12hPrecPrb_A Importance: 0.03\n",
      "Variable: ceil_D       Importance: 0.03\n",
      "Variable: snowPrb_D    Importance: 0.02\n",
      "Variable: 6hQntPrec_A  Importance: 0.02\n",
      "Variable: DISTANCE_GROUP Importance: 0.02\n",
      "Variable: fzRnPrb_D    Importance: 0.01\n",
      "Variable: fzRnPrb_A    Importance: 0.01\n",
      "Variable: snowPrb_A    Importance: 0.01\n",
      "Variable: snow_D       Importance: 0.01\n",
      "Variable: ceil_A       Importance: 0.01\n",
      "Variable: visib_D      Importance: 0.01\n",
      "Variable: visib_A      Importance: 0.01\n",
      "Variable: 12hQntPrec_A Importance: 0.01\n",
      "Variable: 12hQntPrec_D Importance: 0.01\n",
      "Variable: month        Importance: 0.01\n",
      "Variable: snow_A       Importance: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature importance\n",
    "# Get numerical feature importances\n",
    "features = X\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "#importances = list(model_xgb.feature_importances_)\n",
    "#importances = list(gb_clf.feature_importances_)\n",
    "importances = list(clf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:12} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. ],\n",
       "       [1. , 0. ],\n",
       "       [1. , 0. ],\n",
       "       ...,\n",
       "       [0.6, 0.4],\n",
       "       [1. , 0. ],\n",
       "       [0.9, 0.1]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict probabilities\n",
    "#predictions = clf.predict_proba(X_test)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Negative class (0) is most frequent\n",
    "#dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "#y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "dummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "\n",
    "print('Random class-proportional (dummy)\\n', \n",
    "      #classification_report(y_test, y_classprop_predicted, target_names=['0', '1']))\n",
    "      classification_report(y_test, y_classprop_predicted, target_names=['< 15 min', '15-1hr','1hr-2hr','>2 hr']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
